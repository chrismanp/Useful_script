--- linux-4.7.1.old/mm/mprotect.c	2016-08-16 02:35:15.000000000 -0500
+++ linux-4.7.1/mm/mprotect.c	2018-10-27 01:24:07.266053417 -0500
@@ -1,4 +1,4 @@
-/*
+ /*
  *  mm/mprotect.c
  *
  *  (C) Copyright 1994 Linus Torvalds
@@ -76,6 +76,8 @@
 	arch_enter_lazy_mmu_mode();
 	do {
 		oldpte = *pte;
+		//if (pte_val(oldpte) != 0)
+		//  printk("%s %s %d : Address : %lx pte : %lx\n", __FILE__, __func__, __LINE__, addr, (u64) pte_val(oldpte));
 		if (pte_present(oldpte)) {
 			pte_t ptent;
 			bool preserve_write = prot_numa && pte_write(oldpte);
@@ -98,16 +100,20 @@
 
 			ptent = ptep_modify_prot_start(mm, addr, pte);
 			ptent = pte_modify(ptent, newprot);
-			if (preserve_write)
-				ptent = pte_mkwrite(ptent);
-
+			if (preserve_write) {
+			printk("%s %s %d\n", __FILE__, __func__, __LINE__);
+			  ptent = pte_mkwrite(ptent);
+                        }
 			/* Avoid taking write faults for known dirty pages */
 			if (dirty_accountable && pte_dirty(ptent) &&
 					(pte_soft_dirty(ptent) ||
 					 !(vma->vm_flags & VM_SOFTDIRTY))) {
-				ptent = pte_mkwrite(ptent);
+			  printk("%s %s %d\n", __FILE__, __func__, __LINE__);
+			  ptent = pte_mkwrite(ptent);
 			}
 			ptep_modify_prot_commit(mm, addr, pte, ptent);
+                        //if(pte_val(ptent) != 0)
+			//    printk("%s %s %d : Address : %lx ptent : %lx\n", __FILE__, __func__, __LINE__, addr, (u64) pte_val(ptent));   
 			pages++;
 		} else if (IS_ENABLED(CONFIG_MIGRATION)) {
 			swp_entry_t entry = pte_to_swp_entry(oldpte);
@@ -145,6 +151,8 @@
 	unsigned long nr_huge_updates = 0;
 	unsigned long mni_start = 0;
 
+        printk("%s %s %d addr : %lx\n", __FILE__, __func__, __LINE__, addr);
+
 	pmd = pmd_offset(pud, addr);
 	do {
 		unsigned long this_pages;
@@ -181,9 +189,13 @@
 			}
 			/* fall through, the trans huge pmd just split */
 		}
-		this_pages = change_pte_range(vma, pmd, addr, next, newprot,
-				 dirty_accountable, prot_numa);
-		pages += this_pages;
+
+                /* chrisma */
+		//this_pages = change_pte_range(vma, pmd, addr, next, newprot,
+		//		 dirty_accountable, prot_numa);
+		//pages += this_pages;
+                /* chrisma */
+
 	} while (pmd++, addr = next, addr != end);
 
 	if (mni_start)
@@ -207,8 +219,9 @@
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
 			continue;
-		pages += change_pmd_range(vma, pud, addr, next, newprot,
-				 dirty_accountable, prot_numa);
+		/* chrisma */
+		//pages += change_pmd_range(vma, pud, addr, next, newprot,
+		//		 dirty_accountable, prot_numa);
 	} while (pud++, addr = next, addr != end);
 
 	return pages;
@@ -223,7 +236,8 @@
 	unsigned long next;
 	unsigned long start = addr;
 	unsigned long pages = 0;
-
+	
+	//	printk("%s %s %d\n", __FILE__, __func__, __LINE__);
 	BUG_ON(addr >= end);
 	pgd = pgd_offset(mm, addr);
 	flush_cache_range(vma, addr, end);
@@ -232,8 +246,9 @@
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
-		pages += change_pud_range(vma, pgd, addr, next, newprot,
-				 dirty_accountable, prot_numa);
+		// chrisma
+                //pages += change_pud_range(vma, pgd, addr, next, newprot,
+		//		 dirty_accountable, prot_numa);
 	} while (pgd++, addr = next, addr != end);
 
 	/* Only flush the TLB if we actually modified any entries: */
@@ -249,12 +264,16 @@
 		       int dirty_accountable, int prot_numa)
 {
 	unsigned long pages;
-
-	if (is_vm_hugetlb_page(vma))
-		pages = hugetlb_change_protection(vma, start, end, newprot);
-	else
-		pages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);
-
+	//	printk("%s %s %d\n", __FILE__, __func__, __LINE__);
+	if (is_vm_hugetlb_page(vma)){
+	        printk("%s %s %d\n", __FILE__, __func__, __LINE__);
+	        pages = hugetlb_change_protection(vma, start, end, newprot);
+	}
+	else {
+	  //	        printk("%s %s %d\n", __FILE__, __func__, __LINE__);
+	        // chrisma  
+                //pages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);
+        }
 	return pages;
 }
 
@@ -262,6 +281,7 @@
 mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 	unsigned long start, unsigned long end, unsigned long newflags)
 {
+        printk("%s %s %d add : %lx\n", __FILE__, __func__, __LINE__, start);
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long oldflags = vma->vm_flags;
 	long nrpages = (end - start) >> PAGE_SHIFT;
@@ -282,6 +302,7 @@
 	 * even if read-only so there is no need to account for them here
 	 */
 	if (newflags & VM_WRITE) {
+	  printk("%s %s %d add : %lx newflags : %lx VM_WRITE : %lx VM_READ = %lx\n", __FILE__, __func__, __LINE__, start, newflags, VM_WRITE, VM_READ);
 		/* Check space limits when area turns into data. */
 		if (!may_expand_vm(mm, newflags, nrpages) &&
 				may_expand_vm(mm, oldflags, nrpages))
@@ -330,8 +351,10 @@
 	dirty_accountable = vma_wants_writenotify(vma);
 	vma_set_page_prot(vma);
 
-	change_protection(vma, start, end, vma->vm_page_prot,
-			  dirty_accountable, 0);
+	printk("%s %s %d add : %lx newflags : %lx oldflags : %lx\n", __FILE__, __func__, __LINE__, start, newflags, oldflags);
+        // chrisma
+        //change_protection(vma, start, end, vma->vm_page_prot,
+	//			  dirty_accountable, 0);
 
 	/*
 	 * Private VM_LOCKED VMA becoming writable: trigger COW to avoid major
@@ -355,6 +378,7 @@
 SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 		unsigned long, prot)
 {
+        //printk("%s %s %d address : %lx\n", __FILE__, __func__, __LINE__, start);
 	unsigned long nstart, end, tmp, reqprot;
 	struct vm_area_struct *vma, *prev;
 	int error = -EINVAL;
@@ -425,7 +449,7 @@
 			error = -EACCES;
 			goto out;
 		}
-
+		//		printk("%s %s %d\n", __FILE__, __func__, __LINE__);
 		error = security_file_mprotect(vma, reqprot, prot);
 		if (error)
 			goto out;
@@ -433,6 +457,7 @@
 		tmp = vma->vm_end;
 		if (tmp > end)
 			tmp = end;
+		//		printk("%s %s %d\n", __FILE__, __func__, __LINE__);
 		error = mprotect_fixup(vma, &prev, nstart, tmp, newflags);
 		if (error)
 			goto out;
